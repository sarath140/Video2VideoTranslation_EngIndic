{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyM0VAJyWkBrKG1YTp//gDh/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"841c6f7f2a7249d58dec83b07b6b8882":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f8ee31c928b944ec953abb64d8116c52","IPY_MODEL_52045add1ed14339ba5d8507b24c73c5","IPY_MODEL_6eb2813885314e4cb70dd9235b122636"],"layout":"IPY_MODEL_541a3f0deb8e4dd2b0b639c45f4d967f"}},"f8ee31c928b944ec953abb64d8116c52":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5aefb2f62f194d88809517e86dcd1f6f","placeholder":"​","style":"IPY_MODEL_ed4a6bdeb3f8420dbf6d809b1307de53","value":"config.json: 100%"}},"52045add1ed14339ba5d8507b24c73c5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0d3bbdbe1f924067baee15d82d106a96","max":350,"min":0,"orientation":"horizontal","style":"IPY_MODEL_19ec5ef9a1aa4ce786c60e94830f3cab","value":350}},"6eb2813885314e4cb70dd9235b122636":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8a3b3e738eca4b528613bbbcecbf847a","placeholder":"​","style":"IPY_MODEL_e8c752a3f8d94d48899631bfc8943cf2","value":" 350/350 [00:00&lt;00:00, 38.2kB/s]"}},"541a3f0deb8e4dd2b0b639c45f4d967f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5aefb2f62f194d88809517e86dcd1f6f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed4a6bdeb3f8420dbf6d809b1307de53":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0d3bbdbe1f924067baee15d82d106a96":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"19ec5ef9a1aa4ce786c60e94830f3cab":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8a3b3e738eca4b528613bbbcecbf847a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e8c752a3f8d94d48899631bfc8943cf2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"47851ad053d445e59afadd241bb32684":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_496e2ea517c04abe82c0bf1d858f1613","IPY_MODEL_66f1e3f1c62342ed80382411b8b2ee78","IPY_MODEL_faa7132f0ee14bce9d096fe581afc2d9"],"layout":"IPY_MODEL_66b511aa2c564870b416352be1d4a175"}},"496e2ea517c04abe82c0bf1d858f1613":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cf423542d04e4282a74c7b10a41bc8a0","placeholder":"​","style":"IPY_MODEL_b12dca3cf06c40208e06e947a67ddfa1","value":"model.py: 100%"}},"66f1e3f1c62342ed80382411b8b2ee78":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7929ffed1ddd4354a3149bc9660ad7c8","max":5774,"min":0,"orientation":"horizontal","style":"IPY_MODEL_eae2f47393774699b47cc4393d34ff8b","value":5774}},"faa7132f0ee14bce9d096fe581afc2d9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_136e5319c9ac4bc58734d66f96292a3e","placeholder":"​","style":"IPY_MODEL_6d9597dc8ba44bcebd6234a3ebe37dc8","value":" 5.77k/5.77k [00:00&lt;00:00, 566kB/s]"}},"66b511aa2c564870b416352be1d4a175":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf423542d04e4282a74c7b10a41bc8a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b12dca3cf06c40208e06e947a67ddfa1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7929ffed1ddd4354a3149bc9660ad7c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eae2f47393774699b47cc4393d34ff8b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"136e5319c9ac4bc58734d66f96292a3e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6d9597dc8ba44bcebd6234a3ebe37dc8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["!pip install git+https://github.com/ai4bharat/IndicF5.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"Ofif0m1WXfI7","outputId":"a3f4fb30-0f8d-46ad-98bf-62c34861c640","executionInfo":{"status":"ok","timestamp":1762236389246,"user_tz":-330,"elapsed":38926,"user":{"displayName":"Venkata Sarath Chandra Galla ic40044","userId":"08935175430182167547"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/ai4bharat/IndicF5.git\n","  Cloning https://github.com/ai4bharat/IndicF5.git to /tmp/pip-req-build-yn9qb0m0\n","  Running command git clone --filter=blob:none --quiet https://github.com/ai4bharat/IndicF5.git /tmp/pip-req-build-yn9qb0m0\n","  Resolved https://github.com/ai4bharat/IndicF5.git to commit 13f7c4d627cc10111aea8fe9c0039462cacacdc7\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: accelerate>=0.33.0 in /usr/local/lib/python3.12/dist-packages (from f5_tts==0.1.0) (1.11.0)\n","Collecting cached_path (from f5_tts==0.1.0)\n","  Downloading cached_path-1.8.0-py3-none-any.whl.metadata (19 kB)\n","Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from f5_tts==0.1.0) (8.3.0)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (from f5_tts==0.1.0) (4.0.0)\n","Collecting ema_pytorch>=0.5.2 (from f5_tts==0.1.0)\n","  Downloading ema_pytorch-0.7.7-py3-none-any.whl.metadata (689 bytes)\n","Collecting hydra-core>=1.3.0 (from f5_tts==0.1.0)\n","  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n","Requirement already satisfied: jieba in /usr/local/lib/python3.12/dist-packages (from f5_tts==0.1.0) (0.42.1)\n","Requirement already satisfied: librosa in /usr/local/lib/python3.12/dist-packages (from f5_tts==0.1.0) (0.11.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from f5_tts==0.1.0) (3.10.0)\n","Collecting numpy<=1.26.4 (from f5_tts==0.1.0)\n","  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from f5_tts==0.1.0) (0.25.1)\n","Collecting pypinyin (from f5_tts==0.1.0)\n","  Downloading pypinyin-0.55.0-py2.py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from f5_tts==0.1.0) (0.6.2)\n","Requirement already satisfied: soundfile in /usr/local/lib/python3.12/dist-packages (from f5_tts==0.1.0) (0.13.1)\n","Collecting tomli (from f5_tts==0.1.0)\n","  Downloading tomli-2.3.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n","Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from f5_tts==0.1.0) (2.8.0+cu126)\n","Requirement already satisfied: torchaudio>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from f5_tts==0.1.0) (2.8.0+cu126)\n","Collecting torchdiffeq (from f5_tts==0.1.0)\n","  Downloading torchdiffeq-0.2.5-py3-none-any.whl.metadata (440 bytes)\n","Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from f5_tts==0.1.0) (4.67.1)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from f5_tts==0.1.0) (4.57.1)\n","Collecting transformers_stream_generator (from f5_tts==0.1.0)\n","  Downloading transformers-stream-generator-0.0.5.tar.gz (13 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting vocos (from f5_tts==0.1.0)\n","  Downloading vocos-0.1.0-py3-none-any.whl.metadata (4.8 kB)\n","Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (from f5_tts==0.1.0) (0.22.2)\n","Collecting x_transformers>=1.31.14 (from f5_tts==0.1.0)\n","  Downloading x_transformers-2.10.2-py3-none-any.whl.metadata (95 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.8/95.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.33.0->f5_tts==0.1.0) (25.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.33.0->f5_tts==0.1.0) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.33.0->f5_tts==0.1.0) (6.0.3)\n","Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.33.0->f5_tts==0.1.0) (0.36.0)\n","Requirement already satisfied: omegaconf<2.4,>=2.2 in /usr/local/lib/python3.12/dist-packages (from hydra-core>=1.3.0->f5_tts==0.1.0) (2.3.0)\n","Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from hydra-core>=1.3.0->f5_tts==0.1.0) (4.9.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->f5_tts==0.1.0) (3.20.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->f5_tts==0.1.0) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->f5_tts==0.1.0) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->f5_tts==0.1.0) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->f5_tts==0.1.0) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->f5_tts==0.1.0) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->f5_tts==0.1.0) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->f5_tts==0.1.0) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->f5_tts==0.1.0) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->f5_tts==0.1.0) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->f5_tts==0.1.0) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->f5_tts==0.1.0) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->f5_tts==0.1.0) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->f5_tts==0.1.0) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->f5_tts==0.1.0) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->f5_tts==0.1.0) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->f5_tts==0.1.0) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->f5_tts==0.1.0) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->f5_tts==0.1.0) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->f5_tts==0.1.0) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->f5_tts==0.1.0) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->f5_tts==0.1.0) (3.4.0)\n","Requirement already satisfied: einops>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from x_transformers>=1.31.14->f5_tts==0.1.0) (0.8.1)\n","Collecting einx>=0.3.0 (from x_transformers>=1.31.14->f5_tts==0.1.0)\n","  Downloading einx-0.3.0-py3-none-any.whl.metadata (6.9 kB)\n","Collecting loguru (from x_transformers>=1.31.14->f5_tts==0.1.0)\n","  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from cached_path->f5_tts==0.1.0) (2.32.4)\n","Requirement already satisfied: rich<14.0,>=12.1 in /usr/local/lib/python3.12/dist-packages (from cached_path->f5_tts==0.1.0) (13.9.4)\n","Collecting boto3<2.0,>=1.0 (from cached_path->f5_tts==0.1.0)\n","  Downloading boto3-1.40.65-py3-none-any.whl.metadata (6.6 kB)\n","Requirement already satisfied: google-cloud-storage<3.0,>=1.32.0 in /usr/local/lib/python3.12/dist-packages (from cached_path->f5_tts==0.1.0) (2.19.0)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets->f5_tts==0.1.0) (18.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets->f5_tts==0.1.0) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets->f5_tts==0.1.0) (2.2.2)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets->f5_tts==0.1.0) (3.6.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets->f5_tts==0.1.0) (0.70.16)\n","Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa->f5_tts==0.1.0) (3.1.0)\n","Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.12/dist-packages (from librosa->f5_tts==0.1.0) (0.60.0)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from librosa->f5_tts==0.1.0) (1.16.3)\n","Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa->f5_tts==0.1.0) (1.6.1)\n","Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa->f5_tts==0.1.0) (1.5.2)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa->f5_tts==0.1.0) (4.4.2)\n","Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa->f5_tts==0.1.0) (1.8.2)\n","Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa->f5_tts==0.1.0) (1.0.0)\n","Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa->f5_tts==0.1.0) (0.4)\n","Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa->f5_tts==0.1.0) (1.1.2)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile->f5_tts==0.1.0) (2.0.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->f5_tts==0.1.0) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->f5_tts==0.1.0) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->f5_tts==0.1.0) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->f5_tts==0.1.0) (1.4.9)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->f5_tts==0.1.0) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->f5_tts==0.1.0) (3.2.5)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->f5_tts==0.1.0) (2.9.0.post0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->f5_tts==0.1.0) (2024.11.6)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->f5_tts==0.1.0) (0.22.1)\n","Collecting encodec==0.1.1 (from vocos->f5_tts==0.1.0)\n","  Downloading encodec-0.1.1.tar.gz (3.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb->f5_tts==0.1.0) (3.1.45)\n","Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb->f5_tts==0.1.0) (4.5.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb->f5_tts==0.1.0) (5.29.5)\n","Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb->f5_tts==0.1.0) (2.11.10)\n","Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb->f5_tts==0.1.0) (2.42.1)\n","Collecting botocore<1.41.0,>=1.40.65 (from boto3<2.0,>=1.0->cached_path->f5_tts==0.1.0)\n","  Downloading botocore-1.40.65-py3-none-any.whl.metadata (5.7 kB)\n","Collecting jmespath<2.0.0,>=0.7.1 (from boto3<2.0,>=1.0->cached_path->f5_tts==0.1.0)\n","  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n","Collecting s3transfer<0.15.0,>=0.14.0 (from boto3<2.0,>=1.0->cached_path->f5_tts==0.1.0)\n","  Downloading s3transfer-0.14.0-py3-none-any.whl.metadata (1.7 kB)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile->f5_tts==0.1.0) (2.23)\n","Requirement already satisfied: frozendict in /usr/local/lib/python3.12/dist-packages (from einx>=0.3.0->x_transformers>=1.31.14->f5_tts==0.1.0) (2.4.6)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->f5_tts==0.1.0) (3.13.1)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->f5_tts==0.1.0) (4.0.12)\n","Requirement already satisfied: google-auth<3.0dev,>=2.26.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached_path->f5_tts==0.1.0) (2.38.0)\n","Requirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached_path->f5_tts==0.1.0) (2.28.0)\n","Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached_path->f5_tts==0.1.0) (2.4.3)\n","Requirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached_path->f5_tts==0.1.0) (2.7.2)\n","Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached_path->f5_tts==0.1.0) (1.7.1)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate>=0.33.0->f5_tts==0.1.0) (1.2.0)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.0->librosa->f5_tts==0.1.0) (0.43.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb->f5_tts==0.1.0) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb->f5_tts==0.1.0) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb->f5_tts==0.1.0) (0.4.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->f5_tts==0.1.0) (1.17.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->cached_path->f5_tts==0.1.0) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->cached_path->f5_tts==0.1.0) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->cached_path->f5_tts==0.1.0) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->cached_path->f5_tts==0.1.0) (2025.10.5)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich<14.0,>=12.1->cached_path->f5_tts==0.1.0) (4.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich<14.0,>=12.1->cached_path->f5_tts==0.1.0) (2.19.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa->f5_tts==0.1.0) (3.6.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->f5_tts==0.1.0) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->f5_tts==0.1.0) (3.0.3)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets->f5_tts==0.1.0) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets->f5_tts==0.1.0) (2025.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->f5_tts==0.1.0) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->f5_tts==0.1.0) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->f5_tts==0.1.0) (25.4.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->f5_tts==0.1.0) (1.8.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->f5_tts==0.1.0) (6.7.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->f5_tts==0.1.0) (0.4.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->f5_tts==0.1.0) (1.22.0)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->f5_tts==0.1.0) (5.0.2)\n","Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage<3.0,>=1.32.0->cached_path->f5_tts==0.1.0) (1.71.0)\n","Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage<3.0,>=1.32.0->cached_path->f5_tts==0.1.0) (1.26.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage<3.0,>=1.32.0->cached_path->f5_tts==0.1.0) (5.5.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage<3.0,>=1.32.0->cached_path->f5_tts==0.1.0) (0.4.2)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage<3.0,>=1.32.0->cached_path->f5_tts==0.1.0) (4.9.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich<14.0,>=12.1->cached_path->f5_tts==0.1.0) (0.1.2)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.26.1->google-cloud-storage<3.0,>=1.32.0->cached_path->f5_tts==0.1.0) (0.6.1)\n","Downloading ema_pytorch-0.7.7-py3-none-any.whl (9.8 kB)\n","Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading x_transformers-2.10.2-py3-none-any.whl (96 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading cached_path-1.8.0-py3-none-any.whl (37 kB)\n","Downloading pypinyin-0.55.0-py2.py3-none-any.whl (840 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.2/840.2 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tomli-2.3.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (250 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.1/250.1 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torchdiffeq-0.2.5-py3-none-any.whl (32 kB)\n","Downloading vocos-0.1.0-py3-none-any.whl (24 kB)\n","Downloading boto3-1.40.65-py3-none-any.whl (139 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading einx-0.3.0-py3-none-any.whl (102 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.0/103.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading loguru-0.7.3-py3-none-any.whl (61 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading botocore-1.40.65-py3-none-any.whl (14.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Downloading s3transfer-0.14.0-py3-none-any.whl (85 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: f5_tts, transformers_stream_generator, encodec\n","  Building wheel for f5_tts (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for f5_tts: filename=f5_tts-0.1.0-py3-none-any.whl size=63483 sha256=11d6db0f09be084e3ce5e40fc3baca920a691c79c83cb36ce352cc2fc5e28b23\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-6lpqflvj/wheels/e7/40/06/593a34bbe734fb6f1b3d955ab096baf09f8b88dcfbfd92c378\n","  Building wheel for transformers_stream_generator (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for transformers_stream_generator: filename=transformers_stream_generator-0.0.5-py3-none-any.whl size=12426 sha256=ee9a25e446c9fb646f8ae7782ff5272e76d0522f72f35d0997c953f3b4a4b260\n","  Stored in directory: /root/.cache/pip/wheels/a8/58/d2/014cb67c3cc6def738c1b1635dbf4e3dab6fb63aba7070dce0\n","  Building wheel for encodec (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for encodec: filename=encodec-0.1.1-py3-none-any.whl size=45759 sha256=829cd5e463852c18db2fff9cd5bfaa6a74e2a4cf382e6fecb9a920542ad15357\n","  Stored in directory: /root/.cache/pip/wheels/b8/eb/9f/e13610cc46ab39d3199fbabebd1c3e142d44b679526e0f228a\n","Successfully built f5_tts transformers_stream_generator encodec\n","Installing collected packages: tomli, pypinyin, numpy, loguru, jmespath, hydra-core, einx, botocore, s3transfer, x_transformers, torchdiffeq, ema_pytorch, boto3, transformers_stream_generator, encodec, vocos, cached_path, f5_tts\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 2.0.2\n","    Uninstalling numpy-2.0.2:\n","      Successfully uninstalled numpy-2.0.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n","opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n","opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n","jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n","thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n","pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n","jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed boto3-1.40.65 botocore-1.40.65 cached_path-1.8.0 einx-0.3.0 ema_pytorch-0.7.7 encodec-0.1.1 f5_tts-0.1.0 hydra-core-1.3.2 jmespath-1.0.1 loguru-0.7.3 numpy-1.26.4 pypinyin-0.55.0 s3transfer-0.14.0 tomli-2.3.0 torchdiffeq-0.2.5 transformers_stream_generator-0.0.5 vocos-0.1.0 x_transformers-2.10.2\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy"]},"id":"6010621f56124edf8067818e72b6dd80"}},"metadata":{}}]},{"cell_type":"code","source":["# Install pytorch with your CUDA version, e.g.\n","!pip install torch==2.4.0+cu124 torchaudio==2.4.0+cu124 --extra-index-url https://download.pytorch.org/whl/cu124"],"metadata":{"id":"BB-HHF5cr3x4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762236663403,"user_tz":-330,"elapsed":233314,"user":{"displayName":"Venkata Sarath Chandra Galla ic40044","userId":"08935175430182167547"}},"outputId":"5fd71f43-fc30-4d1c-cb3f-7f818b31f8f8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu124\n","Collecting torch==2.4.0+cu124\n","  Downloading https://download.pytorch.org/whl/cu124/torch-2.4.0%2Bcu124-cp312-cp312-linux_x86_64.whl (797.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.2/797.2 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torchaudio==2.4.0+cu124\n","  Downloading https://download.pytorch.org/whl/cu124/torchaudio-2.4.0%2Bcu124-cp312-cp312-linux_x86_64.whl (3.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m121.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0+cu124) (3.20.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0+cu124) (4.15.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0+cu124) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0+cu124) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0+cu124) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0+cu124) (2025.3.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0+cu124) (75.2.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.99 (from torch==2.4.0+cu124)\n","  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_nvrtc_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (24.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.7/24.7 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.4.99 (from torch==2.4.0+cu124)\n","  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_runtime_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.4/883.4 kB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.4.99 (from torch==2.4.0+cu124)\n","  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_cupti_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.0+cu124)\n","  Downloading https://download.pytorch.org/whl/cu124/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cublas-cu12==12.4.2.65 (from torch==2.4.0+cu124)\n","  Downloading https://download.pytorch.org/whl/cu124/nvidia_cublas_cu12-12.4.2.65-py3-none-manylinux2014_x86_64.whl (363.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.0/363.0 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cufft-cu12==11.2.0.44 (from torch==2.4.0+cu124)\n","  Downloading https://download.pytorch.org/whl/cu124/nvidia_cufft_cu12-11.2.0.44-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-curand-cu12==10.3.5.119 (from torch==2.4.0+cu124)\n","  Downloading https://download.pytorch.org/whl/cu124/nvidia_curand_cu12-10.3.5.119-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusolver-cu12==11.6.0.99 (from torch==2.4.0+cu124)\n","  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusolver_cu12-11.6.0.99-py3-none-manylinux2014_x86_64.whl (128.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusparse-cu12==12.3.0.142 (from torch==2.4.0+cu124)\n","  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusparse_cu12-12.3.0.142-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.0+cu124)\n","  Downloading https://download.pytorch.org/whl/cu124/nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nvtx-cu12==12.4.99 (from torch==2.4.0+cu124)\n","  Downloading https://download.pytorch.org/whl/cu124/nvidia_nvtx_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (99 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nvjitlink-cu12==12.4.99 (from torch==2.4.0+cu124)\n","  Downloading https://download.pytorch.org/whl/cu124/nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting triton==3.0.0 (from torch==2.4.0+cu124)\n","  Downloading https://download.pytorch.org/whl/triton-3.0.0-1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.4.0+cu124) (3.0.3)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.4.0+cu124) (1.3.0)\n","Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchaudio\n","  Attempting uninstall: triton\n","    Found existing installation: triton 3.4.0\n","    Uninstalling triton-3.4.0:\n","      Successfully uninstalled triton-3.4.0\n","  Attempting uninstall: nvidia-nvtx-cu12\n","    Found existing installation: nvidia-nvtx-cu12 12.6.77\n","    Uninstalling nvidia-nvtx-cu12-12.6.77:\n","      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n","    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n","  Attempting uninstall: nvidia-nccl-cu12\n","    Found existing installation: nvidia-nccl-cu12 2.27.3\n","    Uninstalling nvidia-nccl-cu12-2.27.3:\n","      Successfully uninstalled nvidia-nccl-cu12-2.27.3\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.7.77\n","    Uninstalling nvidia-curand-cu12-10.3.7.77:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n","    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n","      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n","    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n","    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n","    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n","      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n","    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n","    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n","    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.8.0+cu126\n","    Uninstalling torch-2.8.0+cu126:\n","      Successfully uninstalled torch-2.8.0+cu126\n","  Attempting uninstall: torchaudio\n","    Found existing installation: torchaudio 2.8.0+cu126\n","    Uninstalling torchaudio-2.8.0+cu126:\n","      Successfully uninstalled torchaudio-2.8.0+cu126\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.23.0+cu126 requires torch==2.8.0, but you have torch 2.4.0+cu124 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.2.65 nvidia-cuda-cupti-cu12-12.4.99 nvidia-cuda-nvrtc-cu12-12.4.99 nvidia-cuda-runtime-cu12-12.4.99 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.0.44 nvidia-curand-cu12-10.3.5.119 nvidia-cusolver-cu12-11.6.0.99 nvidia-cusparse-cu12-12.3.0.142 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.4.99 torch-2.4.0+cu124 torchaudio-2.4.0+cu124 triton-3.0.0\n"]}]},{"cell_type":"code","source":["from google.colab import userdata\n","from huggingface_hub import login\n","\n","hf_token = userdata.get('HUGGINGFACE_TOKEN')  # or your secret variable name\n","login(token=hf_token)"],"metadata":{"id":"1LD7WDh2pFxn","executionInfo":{"status":"ok","timestamp":1765182852189,"user_tz":-330,"elapsed":1546,"user":{"displayName":"Venkata Sarath Chandra Galla ic40044","userId":"08935175430182167547"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["#text"],"metadata":{"id":"EaKMyVr4skQq"}},{"cell_type":"code","source":["from transformers import AutoModel\n","import numpy as np\n","import soundfile as sf\n","\n","# Load INF5 from Hugging Face\n","repo_id = \"ai4bharat/IndicF5\"\n","model = AutoModel.from_pretrained(repo_id, trust_remote_code=True)\n","\n","# Generate speech\n","audio = model(\n","    \"नमस्ते! संगीत की तरह जीवन भी खूबसूरत होता है, बस इसे सही ताल में जीना आना चाहिए.\"\n",")\n","\n","# Normalize and save output\n","if audio.dtype == np.int16:\n","    audio = audio.astype(np.float32) / 32768.0\n","sf.write(\"samples/namaste.wav\", np.array(audio, dtype=np.float32), samplerate=24000)"],"metadata":{"id":"jnatUY-Batqx","colab":{"base_uri":"https://localhost:8080/","height":596},"executionInfo":{"status":"error","timestamp":1761822674044,"user_tz":-330,"elapsed":239,"user":{"displayName":"Venkata Sarath Chandra Galla ic40044","userId":"08935175430182167547"}},"outputId":"a0109824-0b91-4121-e0ef-c09ed62f38b0"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ImportError","evalue":"cannot import name 'PreTrainedModel' from 'transformers' (/usr/local/lib/python3.12/dist-packages/transformers/__init__.py)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1084849543.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Load INF5 from Hugging Face\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mrepo_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ai4bharat/IndicF5\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Generate speech\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"quantization_config\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m             config, kwargs = AutoConfig.from_pretrained(\n\u001b[0m\u001b[1;32m    550\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0mreturn_unused_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhas_remote_code\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1346\u001b[0;31m             config_class = get_class_from_dynamic_module(\n\u001b[0m\u001b[1;32m   1347\u001b[0m                 \u001b[0mclass_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode_revision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcode_revision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1348\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/dynamic_module_utils.py\u001b[0m in \u001b[0;36mget_class_from_dynamic_module\u001b[0;34m(class_reference, pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, repo_type, code_revision, **kwargs)\u001b[0m\n\u001b[1;32m    614\u001b[0m         \u001b[0mrepo_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepo_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m     )\n\u001b[0;32m--> 616\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_class_in_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_reload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_download\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/dynamic_module_utils.py\u001b[0m in \u001b[0;36mget_class_in_module\u001b[0;34m(class_name, module_path, force_reload)\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0;31m# reload in both cases, unless the module is already imported and the hash hits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__transformers_module_hash__\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mmodule_hash\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0mmodule_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexec_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__transformers_module_hash__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule_hash\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n","\u001b[0;32m~/.cache/huggingface/modules/transformers_modules/ai4bharat/IndicF5/b82d286220e3070e171f4ef4b4bd047b9a447c9a/model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPreTrainedModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'PreTrainedModel' from 'transformers' (/usr/local/lib/python3.12/dist-packages/transformers/__init__.py)","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["from transformers import AutoModel, AutoTokenizer\n","import numpy as np\n","import soundfile as sf\n","import torch\n","\n","# Load model (disable meta tensors)\n","repo_id = \"ai4bharat/IndicF5\"\n","model = AutoModel.from_pretrained(\n","    \"ai4bharat/IndicF5\",\n","    trust_remote_code=True,\n","    torch_dtype=torch.float32\n",").to_empty(device=\"cpu\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","\n","# # Optional: if the model requires a tokenizer\n","# try:\n","#     tokenizer = AutoTokenizer.from_pretrained(repo_id, trust_remote_code=True)\n","# except:\n","#     tokenizer = None\n","\n","# Generate speech\n","audio = model(\n","    \"नमस्ते! संगीत की तरह जीवन भी खूबसूरत होता है, बस इसे सही ताल में जीना आना चाहिए.\",\n","    ref_audio_path=\"prompts/PAN_F_HAPPY_00001.wav\",\n","    ref_text=\"ਭਹੰਪੀ ਵਿੱਚ ਸਮਾਰਕਾਂ ਦੇ ਭਵਨ ਨਿਰਮਾਣ ਕਲਾ ਦੇ ਵੇਰਵੇ ਗੁੰਝਲਦਾਰ ਅਤੇ ਹੈਰਾਨ ਕਰਨ ਵਾਲੇ ਹਨ, ਜੋ ਮੈਨੂੰ ਖੁਸ਼ ਕਰਦੇ ਹਨ।\"\n",")\n","\n","# Normalize and save\n","audio = np.array(audio, dtype=np.float32)\n","if audio.dtype == np.int16:\n","    audio = audio.astype(np.float32) / 32768.0\n","\n","sf.write(\"samples/namaste.wav\", audio, samplerate=24000)\n","print(\"✅ Saved synthesized audio at samples/namaste.wav\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":873,"referenced_widgets":["841c6f7f2a7249d58dec83b07b6b8882","f8ee31c928b944ec953abb64d8116c52","52045add1ed14339ba5d8507b24c73c5","6eb2813885314e4cb70dd9235b122636","541a3f0deb8e4dd2b0b639c45f4d967f","5aefb2f62f194d88809517e86dcd1f6f","ed4a6bdeb3f8420dbf6d809b1307de53","0d3bbdbe1f924067baee15d82d106a96","19ec5ef9a1aa4ce786c60e94830f3cab","8a3b3e738eca4b528613bbbcecbf847a","e8c752a3f8d94d48899631bfc8943cf2","47851ad053d445e59afadd241bb32684","496e2ea517c04abe82c0bf1d858f1613","66f1e3f1c62342ed80382411b8b2ee78","faa7132f0ee14bce9d096fe581afc2d9","66b511aa2c564870b416352be1d4a175","cf423542d04e4282a74c7b10a41bc8a0","b12dca3cf06c40208e06e947a67ddfa1","7929ffed1ddd4354a3149bc9660ad7c8","eae2f47393774699b47cc4393d34ff8b","136e5319c9ac4bc58734d66f96292a3e","6d9597dc8ba44bcebd6234a3ebe37dc8"]},"id":"tfbPK492opWN","executionInfo":{"status":"error","timestamp":1765182961746,"user_tz":-330,"elapsed":14705,"user":{"displayName":"Venkata Sarath Chandra Galla ic40044","userId":"08935175430182167547"}},"outputId":"8c63efad-2aa9-426e-f166-cb718d485ed9"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"841c6f7f2a7249d58dec83b07b6b8882"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.py:   0%|          | 0.00/5.77k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47851ad053d445e59afadd241bb32684"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Encountered exception while importing f5_tts: No module named 'f5_tts'\n","/usr/local/lib/python3.12/dist-packages/pydub/utils.py:300: SyntaxWarning: invalid escape sequence '\\('\n","  m = re.match('([su]([0-9]{1,2})p?) \\(([0-9]{1,2}) bit\\)$', token)\n","/usr/local/lib/python3.12/dist-packages/pydub/utils.py:301: SyntaxWarning: invalid escape sequence '\\('\n","  m2 = re.match('([su]([0-9]{1,2})p?)( \\(default\\))?$', token)\n","/usr/local/lib/python3.12/dist-packages/pydub/utils.py:310: SyntaxWarning: invalid escape sequence '\\('\n","  elif re.match('(flt)p?( \\(default\\))?$', token):\n","/usr/local/lib/python3.12/dist-packages/pydub/utils.py:314: SyntaxWarning: invalid escape sequence '\\('\n","  elif re.match('(dbl)p?( \\(default\\))?$', token):\n"]},{"output_type":"error","ename":"ImportError","evalue":"This modeling file requires the following packages that were not found in your environment: f5_tts. Run `pip install f5_tts`","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-253251597.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load model (disable meta tensors)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mrepo_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ai4bharat/IndicF5\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m model = AutoModel.from_pretrained(\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;34m\"ai4bharat/IndicF5\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"quantization_config\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m             config, kwargs = AutoConfig.from_pretrained(\n\u001b[0m\u001b[1;32m    550\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0mreturn_unused_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhas_remote_code\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1346\u001b[0;31m             config_class = get_class_from_dynamic_module(\n\u001b[0m\u001b[1;32m   1347\u001b[0m                 \u001b[0mclass_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode_revision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcode_revision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1348\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/dynamic_module_utils.py\u001b[0m in \u001b[0;36mget_class_from_dynamic_module\u001b[0;34m(class_reference, pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, repo_type, code_revision, **kwargs)\u001b[0m\n\u001b[1;32m    602\u001b[0m         \u001b[0mcode_revision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[0;31m# And lastly we get the class inside our newly created module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m     final_module = get_cached_module_file(\n\u001b[0m\u001b[1;32m    605\u001b[0m         \u001b[0mrepo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m         \u001b[0mmodule_file\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".py\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/dynamic_module_utils.py\u001b[0m in \u001b[0;36mget_cached_module_file\u001b[0;34m(pretrained_model_name_or_path, module_file, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, repo_type, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[0;31m# Check we have all the requirements in our environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m     \u001b[0mmodules_needed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_imports\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolved_module_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0;31m# Now we move the module inside our cached dynamic modules.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/dynamic_module_utils.py\u001b[0m in \u001b[0;36mcheck_imports\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_packages\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m         raise ImportError(\n\u001b[0m\u001b[1;32m    261\u001b[0m             \u001b[0;34m\"This modeling file requires the following packages that were not found in your environment: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0;34mf\"{', '.join(missing_packages)}. Run `pip install {' '.join(missing_packages)}`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: This modeling file requires the following packages that were not found in your environment: f5_tts. Run `pip install f5_tts`","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"0c086529","executionInfo":{"status":"error","timestamp":1761820793810,"user_tz":-330,"elapsed":1189,"user":{"displayName":"Venkata Sarath Chandra Galla ic40044","userId":"08935175430182167547"}},"outputId":"ad8f6e60-0bb2-453a-e26c-9cf3f102845e"},"source":["from transformers import AutoModel\n","import numpy as np\n","import soundfile as sf\n","import torch\n","\n","# Determine the device\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Using device: {device}\")\n","\n","# Load model directly to the device\n","repo_id = \"ai4bharat/IndicF5\"\n","model = AutoModel.from_pretrained(\n","    repo_id,\n","    trust_remote_code=True,\n","    torch_dtype=torch.float32,\n",").to(device)\n","\n","\n","# Generate speech\n","audio = model(\n","    \"नमस्ते! संगीत की तरह जीवन भी खूबसूरत होता है, बस इसे सही ताल में जीना आना चाहिए.\",\n","    ref_audio_path=\"prompts/PAN_F_HAPPY_00001.wav\",\n","    ref_text=\"ਭਹੰਪੀ ਵਿੱਚ ਸਮਾਰਕਾਂ ਦੇ ਭਵਨ ਨਿਰਮਾਣ ਕਲਾ ਦੇ ਵੇਰਵੇ ਗੁੰਝਲਦਾਰ ਅਤੇ ਹੈਰਾਨ ਕਰਨ ਵਾਲੇ ਹਨ, ਜੋ ਮੈਨੂੰ ਖੁਸ਼ ਕਰਦੇ ਹਨ।\"\n",")\n","\n","# Normalize and save\n","audio = np.array(audio, dtype=np.float32)\n","if audio.dtype == np.int16:\n","    audio = audio.astype(np.float32) / 32768.0\n","\n","sf.write(\"samples/namaste.wav\", audio, samplerate=24000)\n","print(\"✅ Saved synthesized audio at samples/namaste.wav\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Download Vocos from huggingface charactr/vocos-mel-24khz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.embed.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.embed.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.norm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.0.gamma: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.0.dwconv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.0.dwconv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.0.norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.0.norm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.0.pwconv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.0.pwconv1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.0.pwconv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.0.pwconv2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.1.gamma: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.1.dwconv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.1.dwconv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.1.norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.1.norm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.1.pwconv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.1.pwconv1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.1.pwconv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.1.pwconv2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.2.gamma: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.2.dwconv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.2.dwconv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.2.norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.2.norm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.2.pwconv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.2.pwconv1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.2.pwconv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.2.pwconv2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.3.gamma: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.3.dwconv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.3.dwconv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.3.norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.3.norm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.3.pwconv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.3.pwconv1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.3.pwconv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.3.pwconv2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.4.gamma: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.4.dwconv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.4.dwconv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.4.norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.4.norm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.4.pwconv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.4.pwconv1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.4.pwconv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.4.pwconv2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.5.gamma: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.5.dwconv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.5.dwconv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.5.norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.5.norm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.5.pwconv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.5.pwconv1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.5.pwconv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.5.pwconv2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.6.gamma: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.6.dwconv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.6.dwconv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.6.norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.6.norm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.6.pwconv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.6.pwconv1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.6.pwconv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.6.pwconv2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.7.gamma: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.7.dwconv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.7.dwconv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.7.norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.7.norm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.7.pwconv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.7.pwconv1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.7.pwconv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.convnext.7.pwconv2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.final_layer_norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for backbone.final_layer_norm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for head.out.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2441: UserWarning: for head.out.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n"]},{"output_type":"error","ename":"NotImplementedError","evalue":"Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1593533700.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Load model directly to the device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mrepo_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ai4bharat/IndicF5\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m model = AutoModel.from_pretrained(\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mrepo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    595\u001b[0m                 \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_for_auto_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauto_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_generation_mixin_to_remote_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    598\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mold_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4969\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mContextManagers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_init_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4970\u001b[0m             \u001b[0;31m# Let's make sure we don't run the init function of buffer modules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4971\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4973\u001b[0m         \u001b[0;31m# Make sure to tie the weights correctly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.cache/huggingface/modules/transformers_modules/ai4bharat/IndicF5/b82d286220e3070e171f4ef4b4bd047b9a447c9a/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# Load vocoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_vocoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocoder_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"vocos\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_local\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# Download and load model weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/f5_tts/infer/utils_infer.py\u001b[0m in \u001b[0;36mload_vocoder\u001b[0;34m(vocoder_name, is_local, local_path, device, hf_cache_dir)\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mstate_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencodec_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mvocoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mvocoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mvocoder_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"bigvgan\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1367\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_grad_fn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_grad_fn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgrad_fn\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mout_grad_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1368\u001b[0m             warnings.warn(\n\u001b[0;32m-> 1369\u001b[0;31m                 \u001b[0;34m\"Using a non-full backward hook when outputs are nested in python data structure \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1370\u001b[0m                 \u001b[0;34m\"is deprecated and will be removed in future versions. This hook will be missing \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m                 \u001b[0;34m\"some grad_output.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0mmodifies\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mplace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m         \u001b[0mArguments\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m             \u001b[0mdevice\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptional\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mspecified\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m                 \u001b[0mcopied\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0mmodifies\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mplace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m         \u001b[0mArguments\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m             \u001b[0mdevice\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptional\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mspecified\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m                 \u001b[0mcopied\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    953\u001b[0m         \"\"\"\n\u001b[1;32m    954\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    956\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m         r\"\"\"Move all model parameters and buffers to the CPU.\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1360\u001b[0m                 )\n\u001b[1;32m   1361\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1362\u001b[0;31m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotImplementedError\u001b[0m: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device."]}]},{"cell_type":"code","source":[],"metadata":{"id":"LSBr0PJ9qd_V"},"execution_count":null,"outputs":[]}]}